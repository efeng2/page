# Classifiers (Machine Learning Algorithms)
2023-10-11

## Decision Trees
 - Query to Leaves

A decision tree is a model that makes decisions by asking a series of questions, each leading to further questions or to a final decision (leaf). Each internal node in the tree represents a decision based on a feature, and each leaf node represents the predicted outcome or class. The tree is constructed by recursively splitting the data based on the feature that provides the best separation (usually by minimizing impurity).

Example:
- *Decision Trees:* In a binary classification task, a decision tree might first split the data based on whether a feature is above or below a certain threshold and then continue splitting until reaching a decision for each class.

## Ensemble Method
 - A Collection of (Generally Weak) Models That Are Combined to Create a More Powerful Model

Ensemble methods combine multiple models, often referred to as "weak learners," to create a stronger overall model. These models work together to improve prediction accuracy by averaging or voting on the outputs of individual models. Popular ensemble methods include bagging, boosting, and stacking.

## Random Forest
 - A Collection of Decision Trees. Each Tree Votes and Takes Majority Vote Decision

A random forest is an ensemble method that uses a collection of decision trees to make predictions. Each tree is trained on a random subset of the data, and when making a prediction, the random forest takes the majority vote from all the trees. This method reduces overfitting and increases the model's robustness compared to a single decision tree.

Example:
- *Random Forest:* If the task is to predict whether a customer will buy a product, each decision tree in the forest will vote for a decision (buy or not buy), and the majority vote is taken as the final prediction.

## AdaBoost
 - Weighted Decision Stumps, If Get Right, Decrease Weight, If Get Wrong, Increase Weight

AdaBoost (Adaptive Boosting) is an ensemble method that combines weak classifiers, often decision stumps (single-level decision trees). Initially, all data points have equal weights. After each weak model is trained, the weights are adjusted: if a model correctly classifies a point, the weight is decreased, and if it misclassifies, the weight is increased. The final model is a weighted combination of all weak learners.

Example:
- *AdaBoost:* In a classification task, AdaBoost might first train a weak model that misclassifies many points. It then focuses more on the misclassified points for the next round, creating a stronger classifier by combining multiple weak models.

## Generative
 - Defines a Model for Generating X, Naïve Bayes

Generative models focus on modeling the underlying distribution of the data. They attempt to learn how data is generated by defining a model that describes how the inputs (X) are related to the outputs (Y). A common example of a generative model is Naïve Bayes, which assumes that the features are conditionally independent given the class.

Example:
- *Naïve Bayes:* Given a dataset with features like age, income, and occupation, Naïve Bayes would estimate the likelihood of each class (e.g., "buy" vs. "not buy") given the features by modeling the joint probability distribution of the data.

## Discriminative
 - Only Cares About Defining and Optimizing a Decision Boundary

Discriminative models focus on directly modeling the decision boundary between different classes. Instead of modeling how the data is generated, they focus on learning the mapping from inputs to outputs. Examples include logistic regression, support vector machines (SVM), and discriminative variants of neural networks.

Example:
- *Logistic Regression:* A discriminative model would learn the parameters that define the boundary (e.g., a straight line) separating classes (e.g., "spam" vs. "not spam") by optimizing the decision boundary.

## Weighted k-NN Regression
 - Kernel Regression If All Training Points Are Weighted with Kernel

In weighted k-NN regression, predictions are made by taking the weighted average of the k nearest neighbors, where each neighbor’s weight depends on its distance to the query point. A kernel function is often used to assign weights, with closer neighbors receiving higher weights. This method smooths predictions and can handle nonlinear relationships between the input features and output.

Example:
- *Weighted k-NN Regression:* In predicting house prices, the prediction for a new house might be based on the prices of the nearest houses.

## Kernel
 - Turn Distance Into Weight

A kernel is a function that transforms the input data into a higher-dimensional space to make it easier to find complex patterns. In many machine learning algorithms (e.g., support vector machines, kernel regression), kernels are used to measure similarity between points. The kernel function calculates a "weight" or similarity score based on the distance between data points.

Example:
- *Kernel Trick:* In support vector machines, the kernel trick allows the algorithm to find a non-linear decision boundary by implicitly mapping the data into a higher-dimensional space without explicitly computing the transformation.

## Locality Sensitive Hashing
 - Break the Data into Smaller Bins Based on How Close They Are to Each Other, More Bins → Fewer Points Per Bin → Faster Search → More Likely to Make Errors If We Aren’t Careful

Locality Sensitive Hashing (LSH) is a technique used to speed up nearest neighbor search by hashing similar data points into the same bucket. LSH ensures that similar points are more likely to collide into the same bin, making it faster to search for neighbors. However, this method can lead to errors if the number of bins is too high or if the hash function does not sufficiently capture the similarity.

## Binning
 - Put Data in Bins Based on the Sign of the Score, Efficiency O(n/2)

Binning is a technique used to group data into intervals or "bins" based on certain criteria, such as the sign of a score. It is used to reduce the granularity of the data, making it easier to process and analyze. The efficiency of binning can be seen as O(n/2), where n is the number of data points.

Example:
- *Binning for Classification:* In binary classification, binning might be used to group data points into "positive" and "negative" bins based on the predicted score, simplifying the decision-making process for classification tasks.
