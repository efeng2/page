# 深度学习  
2023-10-11  

## 神经网络 (Neural Network)  
- 由多个感知机构成，容易过拟合  

神经网络由多层互联节点（称为感知机或人工神经元）组成。每个感知机接收输入，通过激活函数处理后传递至下一层。虽然神经网络能够学习数据中的复杂模式，但当感知机数量过多或模型复杂度远超数据集规模时，极易出现过拟合现象。若在小型数据集上训练包含大量感知机的网络，模型可能学习到训练数据特有的噪声和无关模式，导致泛化能力下降。

## 卷积神经网络 (Convolutional Neural Networks)  
- 通过卷积操作整合局部像素信息，减少参数量  

卷积神经网络（CNNs）专为图像数据处理设计。传统神经网络采用全连接方式会导致参数量爆炸，而CNNs通过卷积核在图像局部区域滑动计算，显著降低计算复杂度。通过逐层学习边缘、纹理等局部特征，最终识别出更高层次的抽象模式。

### 典型应用：  
- 在图像分类任务中，浅层CNN可能学习边缘检测，深层则识别更复杂的形状（如圆形或人脸），这种层级结构大幅减少了有效识别所需的参数量。

## 卷积核 (Kernel)  
- 核矩阵与图像局部区域的元素乘积之和  

卷积核（或称滤波器）是卷积运算中使用的小型矩阵。它在图像上滑动，每次计算核矩阵与图像局部区域的元素乘积之和，生成特征图的一个像素值。通过训练学习的卷积核能够自动提取图像中最相关的特征（如边缘、纹理）。

### 运算示例：  
- 对于一个3×3卷积核，其在图像上滑动时，每个位置计算3×3局部区域与核矩阵的逐元素乘积和，输出结果构成新的特征图。

## 池化 (Pooling)  
- 对局部像素进行聚合运算（如取最大值/平均值）  

池化是CNN中的降采样操作，通过压缩图像空间尺寸（高度和宽度）来保留关键信息。常用方法包括最大池化（取局部区域最大值）和平均池化（取局部区域平均值）。池化层能有效减少计算量，并通过降低对图像微小变化的敏感性来防止过拟合。

### 典型应用：  
- 2×2最大池化操作从图像的每个2×2区域选取最大值，在保留显著特征的同时将图像尺寸压缩至1/4。

## 迁移学习 (Transfer Learning)  
- 将任务1中学到的知识迁移到任务2  

迁移学习指利用预训练模型（通常基于大型数据集）通过微调适应新任务的方法。当新任务数据有限时，这种方法尤其有效——模型可以继承预训练获得的基础特征识别能力，只需调整最后几层网络即可适应新任务的特定模式。

### 典型场景：  
- 使用在猫狗分类数据集上预训练的模型，通过微调最后几层，可快速适配到植物物种识别等新任务，大幅降低训练成本。  
