# 机器学习分类器算法  
2023-10-11  

## 决策树 (Decision Trees)  
- 通过查询路径到达叶节点  

决策树是通过一系列特征判断问题最终到达决策结果（叶节点）的模型。每个内部节点代表基于特征的决策判断，叶节点代表最终预测结果。通过递归地选择最能区分数据的特征（通常以最小化不纯度为目标）进行构建。  

应用示例：  
- 在二分类任务中，决策树可能首先根据特征是否超过阈值来划分数据，然后持续分割直到为每个类别做出决策。  

## 集成方法 (Ensemble Method)  
- 组合多个（通常较弱的）模型形成更强模型  

集成方法通过组合多个"弱学习器"来构建更强的整体模型。这些模型通过平均或投票机制协同工作以提高预测准确率。常用方法包括装袋法(bagging)、提升法(boosting)和堆叠法(stacking)。  

## 随机森林 (Random Forest)  
- 决策树的集合，采用多数表决机制  

随机森林是由多棵决策树组成的集成方法。每棵树在数据的随机子集上训练，预测时采用所有树的多数表决结果。相比单棵决策树，这种方法能有效降低过拟合并提高模型鲁棒性。  

应用示例：  
- 预测客户是否购买产品时，森林中的每棵树进行投票表决，最终采用多数结果作为预测结论。  

## 自适应增强 (AdaBoost)  
- 加权决策桩，正确则降低权重，错误则增加权重  

AdaBoost通过组合多个弱分类器（通常是单层决策树）工作。初始所有数据点权重相同，每轮训练后调整权重：正确分类的点降低权重，错误分类的点增加权重。最终模型是所有弱学习器的加权组合。  

应用示例：  
- 在分类任务中，AdaBoost可能首轮训练出错误率较高的弱模型，随后更关注被错误分类的样本，通过多轮迭代组合成强分类器。  

## 生成式模型 (Generative)  
- 建立数据生成模型，如朴素贝叶斯  

生成式模型专注于建模数据的底层分布，试图学习数据生成过程。典型代表是朴素贝叶斯，它假设给定类别时各特征条件独立。  

应用示例：  
- 给定年龄、收入和职业等特征，朴素贝叶斯通过建模联合概率分布来估计"购买"和"不购买"类别的似然概率。  

## 判别式模型 (Discriminative)  
- 仅关注定义和优化决策边界  

判别式模型直接建模类别间的决策边界而非数据生成过程。典型算法包括逻辑回归、支持向量机(SVM)和判别式神经网络。  

应用示例：  
- 逻辑回归通过学习参数来定义区分"垃圾邮件"和"非垃圾邮件"的决策边界（如直线）。  

## 加权k近邻回归 (Weighted k-NN Regression)  
- 若所有训练点都采用核函数加权即为核回归  

该方法通过计算k个最近邻的加权平均值进行预测，权重由查询点的距离决定（常用核函数分配权重）。能处理特征与输出间的非线性关系。  

应用示例：  
- 预测房价时，新房屋的价格预测基于最近邻房屋价格的加权平均。  

## 核方法 (Kernel)  
- 将距离转换为权重  

核函数将数据映射到高维空间以发现复杂模式。在SVM、核回归等算法中用于衡量数据点相似度，基于距离计算权重或相似度得分。  

应用示例：  
- SVM中的核技巧通过隐式高维映射寻找非线性决策边界，而无需显式计算转换过程。  

## 局部敏感哈希 (Locality Sensitive Hashing)  
- 根据数据邻近性分桶，桶越多→每桶数据点越少→搜索越快→但需注意可能增加误差  

LSH通过哈希将相似数据映射到相同桶中来加速近邻搜索。但若分桶过多或哈希函数不能有效捕捉相似性时可能产生误差。  

## 分箱处理 (Binning)  
- 根据分数符号将数据分箱，效率O(n/2)  

该技术通过特定标准（如分数符号）将数据分组到区间箱中，降低数据粒度便于分析处理。  

应用示例：  
- 二分类任务中，可根据预测分数将数据点分入"正例"和"负例"箱，简化分类决策过程。