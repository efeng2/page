# 马尔可夫决策过程中的值迭代与贝尔曼方程
2025-02-25  

值迭代(Value Iteration)是寻找从当前状态到目标状态最佳动作的一种方法。  

## 贝尔曼方程 (Bellman Equations)  
更新方程：  

$$
V_{\pi}(s) = \begin{cases} 
0 & \text{若 } s \in \text{目标状态} \\
\min_{a} Q_{\pi}(s, a) & \text{其他情况}
\end{cases}
$$

$$
Q_{\pi}(s, a) = \sum_{s'} T(s, a, s') \times \big(R(s,a,s') + \gamma V(s')\big)
$$

其中：
- $V_{\pi}(s)$ 表示状态$s$的值函数
- $Q_{\pi}(s,a)$ 表示状态$s$下采取动作$a$的动作值函数  
- $T(s,a,s')$ 是状态转移概率
- $R(s,a,s')$ 是即时奖励
- $\gamma$ 是折扣因子

## 动态规划 (Dynamic Programming)  
用于高效求解马尔可夫决策过程(MDPs)的方法：

### 值迭代算法 (Value Iteration)  
1. 初始化$V_0$（目标状态设为0，其他状态可设为100）  
2. 重复迭代直到最大残差$\delta < \epsilon$：  
$$
V_{k+1}(s) = \max_a \sum_{s'} T(s,a,s')\big[R(s,a,s') + \gamma V_k(s')\big]
$$

### Python实现  
值迭代算法的Python实现参考：  
[链接](https://www.geeksforgeeks.org/implement-value-iteration-in-python/)
